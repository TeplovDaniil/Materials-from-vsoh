{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартные библиотеки\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Работа с данными\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# для модели yolo \n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# для работы с самописной нейросетью\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from IPython.display import Image\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разбиение завершено: 6727 тренировочных, 2883 валидационных файлов.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(images_dir, labels_dir, output_dir, train_ratio=0.7):\n",
    "    # Пути для сохранения разделенных данных\n",
    "    train_images_dir = os.path.join(output_dir, \"train/images\")\n",
    "    train_labels_dir = os.path.join(output_dir, \"train/labels\")\n",
    "    val_images_dir = os.path.join(output_dir, \"val/images\")\n",
    "    val_labels_dir = os.path.join(output_dir, \"val/labels\")\n",
    "    \n",
    "    # Создание папок, если их нет\n",
    "    for folder in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    # Получаем список изображений и перемешиваем\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    # Определяем границу разделения 70/30\n",
    "    split_idx = int(train_ratio * len(image_files))\n",
    "    train_files = image_files[:split_idx]\n",
    "    val_files = image_files[split_idx:]\n",
    "    \n",
    "    # Функция для копирования изображений и соответствующих аннотаций\n",
    "    def move_files(file_list, dst_images, dst_labels):\n",
    "        for file in file_list:\n",
    "            # Перемещение изображения\n",
    "            shutil.copy2(os.path.join(images_dir, file), os.path.join(dst_images, file))\n",
    "            \n",
    "            # Перемещение аннотации (если есть)\n",
    "            label_file = os.path.splitext(file)[0] + \".txt\"\n",
    "            src_label_path = os.path.join(labels_dir, label_file)\n",
    "            dst_label_path = os.path.join(dst_labels, label_file)\n",
    "            \n",
    "            if os.path.exists(src_label_path):\n",
    "                shutil.copy2(src_label_path, dst_label_path)\n",
    "    \n",
    "    # Копируем файлы\n",
    "    move_files(train_files, train_images_dir, train_labels_dir)\n",
    "    move_files(val_files, val_images_dir, val_labels_dir)\n",
    "    \n",
    "    print(f\"Разбиение завершено: {len(train_files)} тренировочных, {len(val_files)} валидационных файлов.\")\n",
    "\n",
    "# Задать пути к данным\n",
    "images_path = \"images2\"\n",
    "labels_path = \"labels_bbox\"\n",
    "output_path = \"data\"\n",
    "\n",
    "# Запустить разбиение\n",
    "split_dataset(images_path, labels_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "file_path = 'labels.txt'\n",
    "\n",
    "# Словарь для хранения классов\n",
    "labels = []\n",
    "\n",
    "# Чтение файла и извлечение классов\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        image, label = line.split()\n",
    "        labels.append(int(label))\n",
    "\n",
    "# Подсчет количества каждого класса\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Извлечение меток и их частот\n",
    "labels_list = list(label_counts.keys())\n",
    "counts_list = list(label_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list.append(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = sorted(set(labels_list))  # Упорядочиваем классы\n",
    "mapping = {old_id: new_id for new_id, old_id in enumerate(labels_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 3: 1,\n",
       " 6: 2,\n",
       " 7: 3,\n",
       " 8: 4,\n",
       " 10: 5,\n",
       " 12: 6,\n",
       " 13: 7,\n",
       " 17: 8,\n",
       " 19: 9,\n",
       " 20: 10,\n",
       " 21: 11,\n",
       " 22: 12,\n",
       " 23: 13,\n",
       " 25: 14,\n",
       " 27: 15,\n",
       " 32: 16,\n",
       " 35: 17,\n",
       " 42: 18,\n",
       " 45: 19,\n",
       " 46: 20,\n",
       " 47: 21,\n",
       " 49: 22,\n",
       " 50: 23,\n",
       " 51: 24,\n",
       " 52: 25,\n",
       " 53: 26,\n",
       " 54: 27,\n",
       " 59: 28,\n",
       " 61: 29,\n",
       " 65: 30,\n",
       " 66: 31,\n",
       " 67: 32,\n",
       " 69: 33,\n",
       " 71: 34,\n",
       " 73: 35,\n",
       " 79: 36,\n",
       " 82: 37,\n",
       " 83: 38,\n",
       " 84: 39,\n",
       " 88: 40,\n",
       " 94: 41,\n",
       " 98: 42,\n",
       " 102: 43,\n",
       " 103: 44,\n",
       " 105: 45,\n",
       " 107: 46,\n",
       " 113: 47,\n",
       " 116: 48,\n",
       " 117: 49,\n",
       " 118: 50,\n",
       " 119: 51,\n",
       " 121: 52,\n",
       " 124: 53,\n",
       " 126: 54,\n",
       " 128: 55,\n",
       " 132: 56,\n",
       " 134: 57,\n",
       " 135: 58,\n",
       " 138: 59,\n",
       " 139: 60,\n",
       " 142: 61,\n",
       " 147: 62,\n",
       " 150: 63,\n",
       " 155: 64,\n",
       " 156: 65,\n",
       " 157: 66,\n",
       " 160: 67,\n",
       " 163: 68,\n",
       " 167: 69,\n",
       " 178: 70,\n",
       " 179: 71,\n",
       " 180: 72,\n",
       " 181: 73,\n",
       " 184: 74,\n",
       " 186: 75,\n",
       " 187: 76,\n",
       " 188: 77,\n",
       " 191: 78,\n",
       " 196: 79,\n",
       " 203: 80,\n",
       " 204: 81,\n",
       " 206: 82,\n",
       " 207: 83,\n",
       " 210: 84,\n",
       " 211: 85,\n",
       " 212: 86,\n",
       " 215: 87,\n",
       " 216: 88,\n",
       " 220: 89,\n",
       " 221: 90,\n",
       " 222: 91,\n",
       " 226: 92,\n",
       " 230: 93,\n",
       " 231: 94,\n",
       " 233: 95,\n",
       " 234: 96,\n",
       " 238: 97,\n",
       " 240: 98,\n",
       " 245: 99,\n",
       " 250: 100,\n",
       " 251: 101,\n",
       " 257: 102,\n",
       " 262: 103,\n",
       " 264: 104,\n",
       " 265: 105,\n",
       " 266: 106,\n",
       " 270: 107,\n",
       " 271: 108,\n",
       " 272: 109,\n",
       " 273: 110,\n",
       " 274: 111,\n",
       " 278: 112,\n",
       " 279: 113,\n",
       " 281: 114,\n",
       " 290: 115,\n",
       " 292: 116,\n",
       " 301: 117,\n",
       " 302: 118,\n",
       " 303: 119,\n",
       " 306: 120,\n",
       " 310: 121,\n",
       " 311: 122,\n",
       " 313: 123,\n",
       " 318: 124,\n",
       " 323: 125,\n",
       " 324: 126,\n",
       " 325: 127,\n",
       " 326: 128,\n",
       " 328: 129,\n",
       " 333: 130,\n",
       " 334: 131,\n",
       " 335: 132,\n",
       " 336: 133,\n",
       " 337: 134,\n",
       " 340: 135,\n",
       " 343: 136,\n",
       " 344: 137,\n",
       " 348: 138,\n",
       " 351: 139,\n",
       " 357: 140,\n",
       " 358: 141,\n",
       " 359: 142,\n",
       " 360: 143,\n",
       " 361: 144,\n",
       " 362: 145,\n",
       " 364: 146,\n",
       " 366: 147,\n",
       " 370: 148,\n",
       " 372: 149,\n",
       " 374: 150,\n",
       " 375: 151,\n",
       " 377: 152,\n",
       " 380: 153,\n",
       " 382: 154,\n",
       " 384: 155,\n",
       " 385: 156,\n",
       " 387: 157,\n",
       " 389: 158,\n",
       " 390: 159,\n",
       " 396: 160,\n",
       " 403: 161,\n",
       " 407: 162,\n",
       " 408: 163,\n",
       " 410: 164,\n",
       " 412: 165,\n",
       " 414: 166,\n",
       " 417: 167,\n",
       " 420: 168,\n",
       " 427: 169,\n",
       " 432: 170,\n",
       " 433: 171,\n",
       " 437: 172,\n",
       " 438: 173,\n",
       " 442: 174,\n",
       " 443: 175,\n",
       " 445: 176,\n",
       " 449: 177,\n",
       " 452: 178,\n",
       " 453: 179,\n",
       " 455: 180,\n",
       " 457: 181,\n",
       " 459: 182,\n",
       " 463: 183,\n",
       " 466: 184,\n",
       " 467: 185,\n",
       " 471: 186,\n",
       " 475: 187,\n",
       " 476: 188,\n",
       " 477: 189,\n",
       " 483: 190,\n",
       " 485: 191,\n",
       " 486: 192,\n",
       " 487: 193,\n",
       " 490: 194,\n",
       " 492: 195,\n",
       " 495: 196,\n",
       " 500: 197,\n",
       " 501: 198,\n",
       " 505: 199,\n",
       " 506: 200,\n",
       " 507: 201,\n",
       " 509: 202,\n",
       " 515: 203,\n",
       " 517: 204,\n",
       " 523: 205,\n",
       " 524: 206,\n",
       " 525: 207,\n",
       " 534: 208,\n",
       " 535: 209,\n",
       " 539: 210,\n",
       " 541: 211,\n",
       " 552: 212,\n",
       " 553: 213,\n",
       " 560: 214,\n",
       " 561: 215,\n",
       " 562: 216,\n",
       " 564: 217,\n",
       " 565: 218,\n",
       " 567: 219,\n",
       " 569: 220,\n",
       " 574: 221,\n",
       " 576: 222,\n",
       " 578: 223,\n",
       " 583: 224,\n",
       " 584: 225,\n",
       " 585: 226,\n",
       " 587: 227,\n",
       " 593: 228,\n",
       " 595: 229,\n",
       " 596: 230,\n",
       " 598: 231,\n",
       " 599: 232,\n",
       " 608: 233,\n",
       " 612: 234,\n",
       " 613: 235,\n",
       " 614: 236,\n",
       " 618: 237,\n",
       " 619: 238,\n",
       " 621: 239,\n",
       " 622: 240,\n",
       " 632: 241,\n",
       " 633: 242,\n",
       " 644: 243,\n",
       " 645: 244,\n",
       " 646: 245,\n",
       " 647: 246,\n",
       " 654: 247,\n",
       " 656: 248,\n",
       " 657: 249,\n",
       " 658: 250,\n",
       " 659: 251,\n",
       " 660: 252,\n",
       " 664: 253,\n",
       " 665: 254,\n",
       " 667: 255,\n",
       " 669: 256,\n",
       " 670: 257,\n",
       " 671: 258,\n",
       " 674: 259,\n",
       " 675: 260,\n",
       " 677: 261,\n",
       " 679: 262,\n",
       " 680: 263,\n",
       " 682: 264,\n",
       " 684: 265,\n",
       " 687: 266,\n",
       " 689: 267,\n",
       " 690: 268,\n",
       " 691: 269,\n",
       " 693: 270,\n",
       " 694: 271,\n",
       " 697: 272,\n",
       " 698: 273,\n",
       " 702: 274,\n",
       " 704: 275,\n",
       " 706: 276,\n",
       " 710: 277,\n",
       " 713: 278,\n",
       " 716: 279,\n",
       " 724: 280,\n",
       " 728: 281,\n",
       " 731: 282,\n",
       " 740: 283,\n",
       " 741: 284,\n",
       " 745: 285,\n",
       " 749: 286,\n",
       " 757: 287,\n",
       " 758: 288,\n",
       " 760: 289,\n",
       " 765: 290,\n",
       " 768: 291,\n",
       " 769: 292,\n",
       " 775: 293,\n",
       " 778: 294,\n",
       " 779: 295,\n",
       " 790: 296,\n",
       " 792: 297,\n",
       " 800: 298,\n",
       " 801: 299,\n",
       " 803: 300,\n",
       " 808: 301,\n",
       " 811: 302,\n",
       " 814: 303,\n",
       " 821: 304,\n",
       " 824: 305,\n",
       " 825: 306,\n",
       " 829: 307,\n",
       " 831: 308,\n",
       " 834: 309,\n",
       " 837: 310,\n",
       " 841: 311,\n",
       " 843: 312,\n",
       " 844: 313,\n",
       " 846: 314,\n",
       " 847: 315,\n",
       " 852: 316,\n",
       " 853: 317,\n",
       " 855: 318,\n",
       " 856: 319,\n",
       " 861: 320,\n",
       " 863: 321,\n",
       " 866: 322,\n",
       " 867: 323,\n",
       " 868: 324,\n",
       " 873: 325,\n",
       " 879: 326,\n",
       " 881: 327,\n",
       " 883: 328,\n",
       " 884: 329,\n",
       " 888: 330,\n",
       " 889: 331,\n",
       " 894: 332,\n",
       " 895: 333,\n",
       " 898: 334,\n",
       " 899: 335,\n",
       " 901: 336,\n",
       " 903: 337,\n",
       " 904: 338,\n",
       " 905: 339,\n",
       " 907: 340,\n",
       " 909: 341,\n",
       " 919: 342,\n",
       " 922: 343,\n",
       " 923: 344,\n",
       " 924: 345,\n",
       " 926: 346,\n",
       " 927: 347,\n",
       " 931: 348,\n",
       " 932: 349,\n",
       " 933: 350,\n",
       " 935: 351,\n",
       " 936: 352,\n",
       " 938: 353,\n",
       " 940: 354,\n",
       " 942: 355,\n",
       " 943: 356,\n",
       " 945: 357,\n",
       " 946: 358,\n",
       " 950: 359,\n",
       " 951: 360,\n",
       " 955: 361,\n",
       " 956: 362,\n",
       " 958: 363,\n",
       " 961: 364,\n",
       " 963: 365,\n",
       " 964: 366,\n",
       " 967: 367,\n",
       " 968: 368,\n",
       " 972: 369,\n",
       " 973: 370,\n",
       " 974: 371,\n",
       " 977: 372,\n",
       " 983: 373,\n",
       " 988: 374,\n",
       " 989: 375,\n",
       " 990: 376,\n",
       " 992: 377,\n",
       " 993: 378,\n",
       " 997: 379,\n",
       " 998: 380,\n",
       " 999: 381,\n",
       " 1002: 382,\n",
       " 1005: 383,\n",
       " 1008: 384,\n",
       " 1014: 385,\n",
       " 1015: 386,\n",
       " 1021: 387,\n",
       " 1024: 388,\n",
       " 1025: 389,\n",
       " 1029: 390,\n",
       " 1030: 391,\n",
       " 1033: 392,\n",
       " 1034: 393,\n",
       " 1041: 394,\n",
       " 1050: 395,\n",
       " 1051: 396,\n",
       " 1054: 397,\n",
       " 1057: 398,\n",
       " 1058: 399,\n",
       " 1062: 400,\n",
       " 1068: 401,\n",
       " 1077: 402,\n",
       " 1078: 403,\n",
       " 1080: 404,\n",
       " 1084: 405,\n",
       " 1086: 406,\n",
       " 1088: 407,\n",
       " 1093: 408,\n",
       " 1101: 409,\n",
       " 1102: 410,\n",
       " 1103: 411,\n",
       " 1104: 412,\n",
       " 1106: 413,\n",
       " 1107: 414,\n",
       " 1113: 415,\n",
       " 1116: 416,\n",
       " 1124: 417,\n",
       " 1128: 418,\n",
       " 1134: 419,\n",
       " 1137: 420,\n",
       " 1145: 421,\n",
       " 1146: 422,\n",
       " 1147: 423,\n",
       " 1149: 424,\n",
       " 1152: 425,\n",
       " 1153: 426,\n",
       " 1155: 427,\n",
       " 1156: 428,\n",
       " 1158: 429,\n",
       " 1161: 430,\n",
       " 1163: 431,\n",
       " 1166: 432,\n",
       " 1170: 433,\n",
       " 1173: 434,\n",
       " 1174: 435,\n",
       " 1175: 436,\n",
       " 1180: 437,\n",
       " 1181: 438,\n",
       " 1182: 439,\n",
       " 1183: 440,\n",
       " 1189: 441,\n",
       " 1193: 442,\n",
       " 1204: 443,\n",
       " 1206: 444,\n",
       " 1209: 445,\n",
       " 1213: 446,\n",
       " 1215: 447,\n",
       " 1216: 448,\n",
       " 1221: 449,\n",
       " 1222: 450,\n",
       " 1223: 451,\n",
       " 1224: 452,\n",
       " 1225: 453,\n",
       " 1226: 454,\n",
       " 1228: 455,\n",
       " 1232: 456,\n",
       " 1241: 457,\n",
       " 1245: 458,\n",
       " 1246: 459,\n",
       " 1248: 460,\n",
       " 1250: 461,\n",
       " 1253: 462,\n",
       " 1261: 463,\n",
       " 1264: 464,\n",
       " 1265: 465,\n",
       " 1269: 466,\n",
       " 1270: 467,\n",
       " 1272: 468,\n",
       " 1276: 469,\n",
       " 1278: 470,\n",
       " 1279: 471,\n",
       " 1281: 472,\n",
       " 1284: 473,\n",
       " 1287: 474,\n",
       " 1289: 475,\n",
       " 1296: 476,\n",
       " 1297: 477,\n",
       " 1301: 478,\n",
       " 1303: 479,\n",
       " 1304: 480,\n",
       " 1305: 481,\n",
       " 1312: 482,\n",
       " 1318: 483,\n",
       " 1322: 484,\n",
       " 1326: 485,\n",
       " 1335: 486,\n",
       " 1336: 487,\n",
       " 1338: 488,\n",
       " 1339: 489,\n",
       " 1343: 490,\n",
       " 1347: 491,\n",
       " 1351: 492,\n",
       " 1352: 493,\n",
       " 1354: 494,\n",
       " 1356: 495,\n",
       " 1361: 496,\n",
       " 1367: 497,\n",
       " 1368: 498,\n",
       " 1370: 499,\n",
       " 1371: 500,\n",
       " 1373: 501,\n",
       " 1374: 502,\n",
       " 1377: 503,\n",
       " 1382: 504,\n",
       " 1383: 505,\n",
       " 1384: 506,\n",
       " 1385: 507,\n",
       " 1388: 508,\n",
       " 1389: 509,\n",
       " 1391: 510,\n",
       " 1396: 511,\n",
       " 1400: 512,\n",
       " 1401: 513,\n",
       " 1402: 514,\n",
       " 1407: 515,\n",
       " 1410: 516,\n",
       " 1412: 517,\n",
       " 1413: 518,\n",
       " 1419: 519,\n",
       " 1422: 520,\n",
       " 1423: 521,\n",
       " 1424: 522,\n",
       " 1425: 523,\n",
       " 1427: 524,\n",
       " 1432: 525,\n",
       " 1435: 526,\n",
       " 1438: 527,\n",
       " 1439: 528,\n",
       " 1441: 529,\n",
       " 1442: 530,\n",
       " 1443: 531,\n",
       " 1446: 532,\n",
       " 1448: 533,\n",
       " 1452: 534,\n",
       " 1456: 535,\n",
       " 1457: 536,\n",
       " 1463: 537,\n",
       " 1467: 538,\n",
       " 1472: 539,\n",
       " 1474: 540,\n",
       " 1475: 541,\n",
       " 1476: 542,\n",
       " 1479: 543,\n",
       " 1480: 544,\n",
       " 1482: 545,\n",
       " 1483: 546,\n",
       " 1486: 547,\n",
       " 1489: 548,\n",
       " 1490: 549,\n",
       " 1491: 550,\n",
       " 1493: 551,\n",
       " 1496: 552,\n",
       " 1499: 553,\n",
       " 1501: 554,\n",
       " 1503: 555,\n",
       " 1507: 556,\n",
       " 1508: 557,\n",
       " 1511: 558,\n",
       " 1512: 559,\n",
       " 1513: 560,\n",
       " 1515: 561,\n",
       " 1517: 562,\n",
       " 1523: 563,\n",
       " 1524: 564,\n",
       " 1525: 565,\n",
       " 1531: 566,\n",
       " 1533: 567,\n",
       " 1534: 568,\n",
       " 1535: 569,\n",
       " 1537: 570,\n",
       " 1539: 571,\n",
       " 1541: 572,\n",
       " 1543: 573,\n",
       " 1546: 574,\n",
       " 1548: 575,\n",
       " 1549: 576,\n",
       " 1554: 577,\n",
       " 1557: 578,\n",
       " 1559: 579,\n",
       " 1560: 580,\n",
       " 1564: 581,\n",
       " 1565: 582,\n",
       " 1568: 583,\n",
       " 1570: 584,\n",
       " 1571: 585,\n",
       " 1573: 586,\n",
       " 1574: 587,\n",
       " 1575: 588,\n",
       " 1577: 589,\n",
       " 1578: 590,\n",
       " 1579: 591,\n",
       " 1581: 592,\n",
       " 1583: 593,\n",
       " 1584: 594,\n",
       " 1585: 595,\n",
       " 1589: 596,\n",
       " 1591: 597,\n",
       " 1601: 598,\n",
       " 1602: 599,\n",
       " 1603: 600,\n",
       " 1608: 601,\n",
       " 1610: 602,\n",
       " 1617: 603,\n",
       " 1619: 604,\n",
       " 1620: 605,\n",
       " 1623: 606,\n",
       " 1633: 607,\n",
       " 1634: 608,\n",
       " 1639: 609,\n",
       " 1640: 610,\n",
       " 1643: 611,\n",
       " 1646: 612,\n",
       " 1648: 613,\n",
       " 1649: 614,\n",
       " 1650: 615,\n",
       " 1652: 616,\n",
       " 1654: 617,\n",
       " 1655: 618,\n",
       " 1659: 619,\n",
       " 1667: 620,\n",
       " 1670: 621,\n",
       " 1674: 622,\n",
       " 1682: 623,\n",
       " 1684: 624,\n",
       " 1686: 625,\n",
       " 1691: 626,\n",
       " 1694: 627,\n",
       " 1695: 628,\n",
       " 1699: 629,\n",
       " 1707: 630,\n",
       " 1710: 631,\n",
       " 1711: 632,\n",
       " 1714: 633,\n",
       " 1720: 634,\n",
       " 1723: 635,\n",
       " 1726: 636,\n",
       " 1728: 637,\n",
       " 1729: 638,\n",
       " 1735: 639,\n",
       " 1739: 640,\n",
       " 1742: 641,\n",
       " 1743: 642,\n",
       " 1746: 643,\n",
       " 1747: 644,\n",
       " 1748: 645,\n",
       " 1753: 646,\n",
       " 1754: 647,\n",
       " 1756: 648,\n",
       " 1757: 649,\n",
       " 1761: 650,\n",
       " 1762: 651,\n",
       " 1763: 652,\n",
       " 1765: 653,\n",
       " 1776: 654,\n",
       " 1778: 655,\n",
       " 1780: 656,\n",
       " 1781: 657,\n",
       " 1785: 658,\n",
       " 1786: 659,\n",
       " 1793: 660,\n",
       " 1795: 661,\n",
       " 1799: 662,\n",
       " 1803: 663,\n",
       " 1804: 664,\n",
       " 1806: 665,\n",
       " 1810: 666,\n",
       " 1811: 667,\n",
       " 1812: 668,\n",
       " 1813: 669,\n",
       " 1815: 670,\n",
       " 1817: 671,\n",
       " 1821: 672,\n",
       " 1824: 673,\n",
       " 1825: 674,\n",
       " 1828: 675,\n",
       " 1833: 676,\n",
       " 1834: 677,\n",
       " 1839: 678,\n",
       " 1840: 679,\n",
       " 1852: 680,\n",
       " 1854: 681,\n",
       " 1856: 682,\n",
       " 1857: 683,\n",
       " 1860: 684,\n",
       " 1861: 685,\n",
       " 1862: 686,\n",
       " 1864: 687,\n",
       " 1865: 688,\n",
       " 1871: 689,\n",
       " 1873: 690,\n",
       " 1879: 691,\n",
       " 1880: 692,\n",
       " 1883: 693,\n",
       " 1885: 694,\n",
       " 1887: 695,\n",
       " 1889: 696,\n",
       " 1890: 697,\n",
       " 1895: 698,\n",
       " 1900: 699,\n",
       " 1903: 700,\n",
       " 1904: 701,\n",
       " 1907: 702,\n",
       " 1910: 703,\n",
       " 1913: 704,\n",
       " 1914: 705,\n",
       " 1916: 706,\n",
       " 1923: 707,\n",
       " 1924: 708,\n",
       " 1925: 709,\n",
       " 1929: 710,\n",
       " 1935: 711,\n",
       " 1936: 712,\n",
       " 1938: 713,\n",
       " 1939: 714,\n",
       " 1940: 715,\n",
       " 1942: 716,\n",
       " 1943: 717,\n",
       " 1944: 718,\n",
       " 1945: 719,\n",
       " 1946: 720,\n",
       " 1947: 721,\n",
       " 1956: 722,\n",
       " 1957: 723,\n",
       " 1959: 724,\n",
       " 1960: 725,\n",
       " 1961: 726,\n",
       " 1966: 727,\n",
       " 1968: 728,\n",
       " 1970: 729,\n",
       " 1974: 730,\n",
       " 1976: 731,\n",
       " 1982: 732,\n",
       " 1984: 733,\n",
       " 1985: 734,\n",
       " 1986: 735,\n",
       " 1989: 736,\n",
       " 1990: 737,\n",
       " 1991: 738,\n",
       " 1994: 739,\n",
       " 1995: 740,\n",
       " 1996: 741,\n",
       " 1997: 742,\n",
       " 2006: 743,\n",
       " 2009: 744,\n",
       " 2010: 745,\n",
       " 2012: 746,\n",
       " 2013: 747,\n",
       " 2015: 748,\n",
       " 2018: 749,\n",
       " 2019: 750,\n",
       " 2024: 751,\n",
       " 2026: 752,\n",
       " 2029: 753,\n",
       " 2030: 754,\n",
       " 2031: 755,\n",
       " 2033: 756,\n",
       " 2049: 757,\n",
       " 2054: 758,\n",
       " 2058: 759,\n",
       " 2063: 760,\n",
       " 2064: 761,\n",
       " 2066: 762,\n",
       " 2067: 763,\n",
       " 2068: 764,\n",
       " 2069: 765,\n",
       " 2070: 766,\n",
       " 2072: 767,\n",
       " 2074: 768,\n",
       " 2078: 769,\n",
       " 2079: 770,\n",
       " 2081: 771,\n",
       " 2084: 772,\n",
       " 2085: 773,\n",
       " 2086: 774,\n",
       " 2088: 775,\n",
       " 2090: 776,\n",
       " 2092: 777,\n",
       " 2093: 778,\n",
       " 2095: 779,\n",
       " 2097: 780,\n",
       " 2098: 781,\n",
       " 2101: 782,\n",
       " 2105: 783,\n",
       " 2106: 784,\n",
       " 2111: 785,\n",
       " 2114: 786,\n",
       " 2116: 787,\n",
       " 2118: 788,\n",
       " 2120: 789,\n",
       " 2121: 790,\n",
       " 2125: 791,\n",
       " 2126: 792,\n",
       " 2129: 793,\n",
       " 2130: 794,\n",
       " 2134: 795,\n",
       " 2135: 796,\n",
       " 2136: 797,\n",
       " 2144: 798,\n",
       " 2146: 799,\n",
       " 2150: 800,\n",
       " 2153: 801,\n",
       " 2156: 802,\n",
       " 2161: 803,\n",
       " 2173: 804,\n",
       " 2174: 805,\n",
       " 2175: 806,\n",
       " 2176: 807,\n",
       " 2184: 808,\n",
       " 2191: 809,\n",
       " 2192: 810,\n",
       " 2193: 811,\n",
       " 2199: 812,\n",
       " 2201: 813,\n",
       " 2202: 814,\n",
       " 2204: 815,\n",
       " 2206: 816,\n",
       " 2208: 817,\n",
       " 2209: 818,\n",
       " 2213: 819,\n",
       " 2214: 820,\n",
       " 2218: 821,\n",
       " 2219: 822,\n",
       " 2221: 823,\n",
       " 2224: 824,\n",
       " 2225: 825,\n",
       " 2228: 826,\n",
       " 2231: 827,\n",
       " 2232: 828,\n",
       " 2235: 829,\n",
       " 2236: 830,\n",
       " 2238: 831,\n",
       " 2241: 832,\n",
       " 2242: 833,\n",
       " 2243: 834,\n",
       " 2249: 835,\n",
       " 2253: 836,\n",
       " 2255: 837,\n",
       " 2257: 838,\n",
       " 2259: 839,\n",
       " 2260: 840,\n",
       " 2263: 841,\n",
       " 2267: 842,\n",
       " 2268: 843,\n",
       " 2269: 844,\n",
       " 2275: 845,\n",
       " 2280: 846,\n",
       " 2285: 847,\n",
       " 2286: 848,\n",
       " 2290: 849,\n",
       " 2291: 850,\n",
       " 2292: 851,\n",
       " 2296: 852,\n",
       " 2297: 853,\n",
       " 2301: 854,\n",
       " 2304: 855,\n",
       " 2307: 856,\n",
       " 2309: 857,\n",
       " 2313: 858,\n",
       " 2314: 859,\n",
       " 2316: 860,\n",
       " 2318: 861,\n",
       " 2323: 862,\n",
       " 2325: 863,\n",
       " 2330: 864,\n",
       " 2331: 865,\n",
       " 2332: 866,\n",
       " 2334: 867,\n",
       " 2335: 868,\n",
       " 2337: 869,\n",
       " 2349: 870,\n",
       " 2352: 871,\n",
       " 2355: 872,\n",
       " 2356: 873,\n",
       " 2358: 874,\n",
       " 2362: 875,\n",
       " 2364: 876,\n",
       " 2366: 877,\n",
       " 2368: 878,\n",
       " 2382: 879,\n",
       " 2383: 880,\n",
       " 2384: 881,\n",
       " 2385: 882,\n",
       " 2386: 883,\n",
       " 2389: 884,\n",
       " 2391: 885,\n",
       " 2392: 886,\n",
       " 2394: 887,\n",
       " 2400: 888,\n",
       " 2403: 889,\n",
       " 2406: 890,\n",
       " 2409: 891,\n",
       " 2414: 892,\n",
       " 2417: 893,\n",
       " 2419: 894,\n",
       " 2425: 895,\n",
       " 2426: 896,\n",
       " 2427: 897,\n",
       " 2429: 898,\n",
       " 2432: 899,\n",
       " 2433: 900,\n",
       " 2439: 901,\n",
       " 2440: 902,\n",
       " 2441: 903,\n",
       " 2443: 904,\n",
       " 2446: 905,\n",
       " 2450: 906,\n",
       " 2454: 907,\n",
       " 2460: 908,\n",
       " 2461: 909,\n",
       " 2463: 910,\n",
       " 2464: 911,\n",
       " 2469: 912,\n",
       " 2472: 913,\n",
       " 2473: 914,\n",
       " 2475: 915,\n",
       " 2477: 916,\n",
       " 2484: 917,\n",
       " 2494: 918,\n",
       " 2502: 919,\n",
       " 2503: 920,\n",
       " 2505: 921,\n",
       " 2507: 922,\n",
       " 2508: 923,\n",
       " 2509: 924,\n",
       " 2510: 925,\n",
       " 2512: 926,\n",
       " 2513: 927,\n",
       " 2516: 928,\n",
       " 2518: 929,\n",
       " 2520: 930,\n",
       " 2522: 931,\n",
       " 2526: 932,\n",
       " 2529: 933,\n",
       " 2532: 934,\n",
       " 2533: 935,\n",
       " 2535: 936,\n",
       " 2537: 937,\n",
       " 2543: 938,\n",
       " 2545: 939,\n",
       " 2548: 940,\n",
       " 2550: 941,\n",
       " 2558: 942,\n",
       " 2559: 943,\n",
       " 2560: 944,\n",
       " 2562: 945,\n",
       " 2571: 946,\n",
       " 2576: 947,\n",
       " 2577: 948,\n",
       " 2579: 949,\n",
       " 2582: 950,\n",
       " 2583: 951,\n",
       " 2588: 952,\n",
       " 2602: 953,\n",
       " 2606: 954,\n",
       " 2608: 955,\n",
       " 2611: 956,\n",
       " 2613: 957,\n",
       " 2615: 958,\n",
       " 2617: 959,\n",
       " 2630: 960,\n",
       " 2636: 961,\n",
       " 2638: 962,\n",
       " 2642: 963,\n",
       " 2649: 964,\n",
       " 2650: 965,\n",
       " 2652: 966,\n",
       " 2654: 967,\n",
       " 2656: 968,\n",
       " 2660: 969,\n",
       " 2667: 970,\n",
       " 2671: 971,\n",
       " 2674: 972,\n",
       " 2680: 973,\n",
       " 2683: 974,\n",
       " 2688: 975,\n",
       " 2689: 976,\n",
       " 2701: 977,\n",
       " 2703: 978,\n",
       " 2705: 979,\n",
       " 2706: 980,\n",
       " 2710: 981,\n",
       " 2711: 982,\n",
       " 2713: 983,\n",
       " 2714: 984,\n",
       " 2715: 985,\n",
       " 2721: 986,\n",
       " 2722: 987,\n",
       " 2724: 988,\n",
       " 2725: 989,\n",
       " 2727: 990,\n",
       " 2728: 991,\n",
       " 2732: 992,\n",
       " 2734: 993,\n",
       " 2744: 994,\n",
       " 2746: 995,\n",
       " 2747: 996,\n",
       " 2749: 997,\n",
       " 2753: 998,\n",
       " 2754: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dir = \"D:\\\\data science\\\\B\\\\data\\\\train\\\\labels\"\n",
    "\n",
    "for label_file in os.listdir(labels_dir):\n",
    "    file_path = os.path.join(labels_dir, label_file)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        class_id = int(parts[0])\n",
    "        if class_id in mapping:\n",
    "            parts[0] = str(mapping[class_id])  # Заменяем старый ID на новый\n",
    "            new_lines.append(\" \".join(parts))\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.writelines(\"\\n\".join(new_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dir = \"D:\\\\data science\\\\B\\\\data\\\\val\\\\labels\"\n",
    "\n",
    "for label_file in os.listdir(labels_dir):\n",
    "    file_path = os.path.join(labels_dir, label_file)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        class_id = int(parts[0])\n",
    "        if class_id in mapping:\n",
    "            parts[0] = str(mapping[class_id])  # Заменяем старый ID на новый\n",
    "            new_lines.append(\" \".join(parts))\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.writelines(\"\\n\".join(new_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YAML обновлен!\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'train': 'data/train/images',\n",
    "    'val': 'data/val/images',\n",
    "    'nc': len(labels_list),  # Должно быть 3483\n",
    "    'names': [str(x) for x in labels_list],  # Список имен классов (в порядке от 0 до 3482)\n",
    "    'train_labels': 'data/train/labels',\n",
    "    'val_labels': 'data/val/labels'\n",
    "}\n",
    "\n",
    "# Сохранение в YAML\n",
    "import yaml\n",
    "with open('yolo_config_fixed.yaml', 'w') as file:\n",
    "    yaml.dump(data, file, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(\"✅ YAML обновлен!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "\n",
    "# # Пример данных для конфигурации YOLO\n",
    "# data = {\n",
    "#     'train': 'data/train/images',  # путь к обучающим изображениям\n",
    "#     'val': 'data/val/images',  # путь к изображениям для валидации\n",
    "#     'nc': len(labels_list),  # количество классов\n",
    "#     'names': labels_list,  # имена классов\n",
    "#     'train_labels': 'data/train/labels',  # путь к меткам обучающих данных\n",
    "#     'val_labels': 'data/val/labels'  # путь к меткам валидационных данных\n",
    "# }\n",
    "\n",
    "# # Сохранение данных в YAML файл\n",
    "# with open('yolo_config2test.yaml', 'w') as file:\n",
    "#     yaml.dump(data, file, default_flow_style=True, allow_unicode=True)\n",
    "\n",
    "# print(\"YAML файл успешно создан!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, data_yaml):\n",
    "    model = YOLO(model_name)\n",
    "    training_results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=5, # число эпох для обучения\n",
    "        imgsz=240, # размер изображения для обучения\n",
    "        batch=16, # размер батча для обучения\n",
    "        device=0, # номер девайса для обучения\n",
    "        single_cls=False # для обучения с учетом классов на основании data.yaml\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.75 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.12.7 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=D:\\data science\\B\\images_v2\\yolov8m.pt, data=D:\\data science\\B\\yolo_config_fixed.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=240, save=True, save_period=-1, val_period=1, cache=False, device=0, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train2\n",
      "Overriding model.yaml nc=80 with nc=3484\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   5792932  ultralytics.nn.modules.head.Detect           [3484, [192, 384, 576]]       \n",
      "Model summary: 295 layers, 27873556 parameters, 27873540 gradients, 90.3 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "WARNING ⚠️ imgsz=[240] must be multiple of max stride 32, updating to [256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\data science\\B\\data\\train\\labels.cache... 9356 images, 2282 backgrounds, 5 corrupt: 100%|██████████| 9357/9357 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ D:\\data science\\B\\data\\train\\images\\frame_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        529         178         133         133]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ D:\\data science\\B\\data\\train\\images\\frame_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        296         114         158         158]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ D:\\data science\\B\\data\\train\\images\\frame_12.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        339         120         158         158]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ D:\\data science\\B\\data\\train\\images\\frame_14.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        429         141         139         139]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ D:\\data science\\B\\data\\train\\images\\frame_16.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        286         132         149         149]\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
      "UserWarning: Got processor for bboxes, but no transform to process it.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata science\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mimages_v2\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43myolov8m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata science\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43myolo_config_fixed.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model_name, data_yaml)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model_name, data_yaml):\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m YOLO(model_name)\n\u001b[1;32m----> 3\u001b[0m     training_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# число эпох для обучения\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# размер изображения для обучения\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# размер батча для обучения\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# номер девайса для обучения\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# для обучения с учетом классов на основании data.yaml\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\engine\\model.py:657\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 657\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:213\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:327\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[0;32m    330\u001b[0m nw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m*\u001b[39m nb), \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:291\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Dataloaders\u001b[39;00m\n\u001b[0;32m    290\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(world_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:55\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[1;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[0;32m     53\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     54\u001b[0m workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\data\\build.py:114\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[1;34m(dataset, batch, workers, shuffle, rank)\u001b[0m\n\u001b[0;32m    112\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m    113\u001b[0m generator\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m6148914691236517205\u001b[39m \u001b[38;5;241m+\u001b[39m RANK)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInfiniteDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIN_MEMORY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollate_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_init_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\ultralytics\\data\\build.py:40\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_sampler\u001b[39m\u001b[38;5;124m\"\u001b[39m, _RepeatSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler))\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\"D:\\\\data science\\\\B\\\\images_v2\\\\yolov8m.pt\", \"D:\\\\data science\\\\B\\\\yolo_config_fixed.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(r'D:\\data science\\B\\runs\\detect\\train\\weights\\best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\data science\\B\\data\\val\\images\\000008.jpg: 256x192 (no detections), 55.4ms\n",
      "Speed: 1.0ms preprocess, 55.4ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 192)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = model.predict(r\"D:\\data science\\B\\data\\val\\images\\000008.jpg\")\n",
    "img_masks = result[0].plot(boxes=True, labels=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sv.plot_image(img_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"output.jpg\", img_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = r\"D:\\data science\\B\\images2\"\n",
    "LABELS_FOLDER = r\"D:\\data science\\B\\labels_bbox\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image)  # Убираем unsqueeze(0), чтобы было 3D\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, image_folder, labels_folder):\n",
    "        self.image_folder = image_folder\n",
    "        self.labels_folder = labels_folder\n",
    "        self.image_files = os.listdir(image_folder)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_name = self.image_files[idx]\n",
    "        img2_name = np.random.choice(self.image_files)\n",
    "\n",
    "        img1 = self.transform(Image.open(os.path.join(self.image_folder, img1_name)).convert('RGB'))\n",
    "        img2 = self.transform(Image.open(os.path.join(self.image_folder, img2_name)).convert('RGB'))\n",
    "\n",
    "        with open(os.path.join(self.labels_folder, img1_name.replace('.jpg', '.txt')), 'r') as f:\n",
    "            label1 = int(f.readline().split()[0])\n",
    "        \n",
    "        with open(os.path.join(self.labels_folder, img2_name.replace('.jpg', '.txt')), 'r') as f:\n",
    "            label2 = int(f.readline().split()[0])\n",
    "\n",
    "        same_class = torch.tensor(1.0 if label1 == label2 else 0.0)\n",
    "        return img1, img2, same_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(512, 256)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        out1 = self.cnn(img1)\n",
    "        out2 = self.cnn(img2)\n",
    "        combined = torch.cat((out1, out2), dim=1)\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset = SiameseDataset(IMAGE_FOLDER, LABELS_FOLDER)\n",
    "dataloader = DataLoader(siamese_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = SiameseNetwork().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f86bcd906e49ac9df653c6e24c4c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.0143\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFont\n",
    "from tqdm.notebook import tqdm\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for img1, img2, labels in tqdm(dataloader):\n",
    "        img1, img2, labels = img1.cuda(), img2.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img1, img2).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"siamese_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image2(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0) # Добавляем batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER2 = r\"D:\\data science\\B\\images2\"\n",
    "LABELS_FOLDER2 = r\"D:\\data science\\B\\labels_bbox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e5abecea7744508bb81bdc41b858ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9610 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изображение принадлежит к классу: 407\n"
     ]
    }
   ],
   "source": [
    "def predict_class(test_img_path, model_path=\"siamese_model.pth\"):\n",
    "    model = SiameseNetwork().cuda()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    test_img = load_image2(test_img_path).cuda()\n",
    "    max_similarity = 0\n",
    "    best_class = None\n",
    "    \n",
    "    for img_name in tqdm(os.listdir(IMAGE_FOLDER2)):\n",
    "        img_path = os.path.join(IMAGE_FOLDER2, img_name)\n",
    "        label_path = os.path.join(LABELS_FOLDER2, img_name.replace('.jpg', '.txt'))\n",
    "        \n",
    "        ref_img = load_image2(img_path).cuda()\n",
    "        with open(label_path, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            label = int(first_line.split()[0]) if first_line else 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            similarity = model(test_img, ref_img).item()\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_class = label\n",
    "    \n",
    "    return best_class\n",
    "\n",
    "# Пример предсказания\n",
    "img_test =r\"D:\\data science\\world_skils\\osn\\images_v2\\images\\000008.jpg\"\n",
    "predicted_class = predict_class(img_test)\n",
    "print(f\"Изображение принадлежит к классу: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f3aad1b6b44a459b17b91f8dfcba06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def validate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in tqdm(dataloader):\n",
    "            img1, img2, labels = img1.cuda(), img2.cuda(), labels.cuda()\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        avg_loss = val_loss / len(dataloader)\n",
    "        accuracy = correct / total\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "# Вызов функции валидации и построение графика\n",
    "val_losses, val_accuracies = validate_model(model, dataloader, criterion)\n",
    "\n",
    "def plot_metrics(val_losses, val_accuracies):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    ax2.plot(val_accuracies, 'b-', label='Validation Accuracy')\n",
    "    \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='r')\n",
    "    ax2.set_ylabel('Accuracy', color='b')\n",
    "    \n",
    "    plt.title('Validation Metrics')\n",
    "    fig.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(val_losses, val_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
